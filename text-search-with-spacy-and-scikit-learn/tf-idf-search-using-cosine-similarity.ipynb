{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all relevant Python libraries and a SpaCy language model.\n",
    "import json\n",
    "import itertools\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['title', 'text', 'url', 'tokenized_text', 'tf_idf'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the tokenized text in your new dataset from milestone. \n",
    "# Each document dictionary should now include a new key-value pair with the lemmatized text of the articles.\n",
    "with open(\"./wiki_data.json\") as f:\n",
    "    wiki_data = json.load(f)\n",
    "wiki_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['winter', '1', 'scholar', 'visit', '1854', 'set', 'worshipful', '12']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a corpus vocabulary. \n",
    "# It should simply be a list of unique tokens in the provided set of documents. \n",
    "tokenized_texts = [t[\"tokenized_text\"] for t in wiki_data]\n",
    "vocab = list(set(itertools.chain(*tokenized_texts)))\n",
    "\n",
    "with open(\"vocab.json\", \"w\") as f:\n",
    "    json.dump(vocab, f)\n",
    "vocab[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pandemic', 7),\n",
       " ('people', 5),\n",
       " ('disease', 4),\n",
       " ('number', 4),\n",
       " ('spread', 2),\n",
       " ('large', 2),\n",
       " ('region', 2),\n",
       " ('worldwide', 2)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count how many times each unique tokens appears in the corpus, \n",
    "# you will need these counts for the next step.\n",
    "docs_count = []\n",
    "for doc in wiki_data:\n",
    "    docs_count.append(Counter(doc[\"tokenized_text\"]))\n",
    "docs_count[0].most_common(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_with_token = {}\n",
    "for token in vocab:\n",
    "    docs_with_token[token] = sum([1 for doc in docs_count if token in doc.keys()])\n",
    "docs_with_token[\"pandemic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Tf-Idf vectors for every article in the dataset\n",
    "# and add the add these vectors to the article dictionaries.\n",
    "# You should end up the same list a list of dictionaries as before,\n",
    "# but with a new key-value pair containing Tf-Idf vectors:\n",
    "#\n",
    "# title: Title of a Wikipedia article the text is taken from.\n",
    "# text: Wikipedia article text (in this dataset we included only the summary).\n",
    "# tokenized_text: Tokenized Wikipedia article text.\n",
    "# url: Link to the Wikipedia article.\n",
    "# tf_idfs: Tf_Idf vector.\n",
    "n_wiki_data = len(wiki_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(doc, tokenized_text, vocab):\n",
    "    tfidf_vec = []\n",
    "    n_tokenized_text = len(tokenized_text)\n",
    "    for token in vocab:\n",
    "        # Compute a term frequency (Tf) per document\n",
    "        tf = doc[token] / n_tokenized_text\n",
    "        \n",
    "        # Compute a log of inverse document frequency (Idf) per document\n",
    "        idf = np.log(n_wiki_data / docs_with_token[token])\n",
    "\n",
    "        tfidf = tf * idf\n",
    "        tfidf_vec.append(tfidf)\n",
    "    return tfidf_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, doc in enumerate(docs_count):\n",
    "    # Update the original list of dictionaries by adding a new field to each document dictionary called tf_idf, \n",
    "    # containing a list of Tf-Idf values for the words in vocabulary.\n",
    "    wiki_data[i][\"tf_idf\"] = vectorize(doc, wiki_data[i][\"tokenized_text\"], vocab)\n",
    "    \n",
    "wiki_data[0][\"tf_idf\"][:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this new list of dictionaries as a JSON file.\n",
    "with open(\"wiki_data.json\", \"w\") as f:\n",
    "    json.dump(wiki_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can try to search our list of dictionaries using this Tf-Idf field using existing tools for similarity. \n",
    "# We suggest to use Scikit-Learn library and its cosine_similarity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [w for w in doc if not (w.is_stop or w.is_punct or len(w.lemma_.strip()) == 0)]\n",
    "    return [t.lemma_ for t in tokens if t.dep_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tfidf(query, docs):\n",
    "    # Vectorize query\n",
    "    query_tkn = tokenizer(query)\n",
    "    query_vec = vectorize(Counter(query_tkn), query_tkn, vocab)\n",
    "    query_arr = np.array(query_vec)\n",
    "    \n",
    "    # Create a function to calculate cosine similarity between each document's Tf-Idf array\n",
    "    # and the Tf-Idf array of the query.\n",
    "    rankings = []\n",
    "    for doc in docs:\n",
    "        doc_arr = np.array(doc[\"tf_idf\"])\n",
    "        rank = cosine_similarity(query_arr.reshape(1,-1), doc_arr.reshape(1, -1))[0][0]\n",
    "        if rank <= 0:\n",
    "            continue\n",
    "        rankings.append((rank, doc[\"title\"]))\n",
    "\n",
    "    # Return the dictionary sorted by the cosine similarity value in reverse order.\n",
    "    return sorted(rankings, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.12086063518833431, 'COVID-19 pandemic'),\n",
       " (0.06488220500470956, 'Pandemic'),\n",
       " (0.06114716796110625, 'Crimson Contagion'),\n",
       " (0.05408174997825669, 'Disease X'),\n",
       " (0.04268474318671646, 'Pandemic Severity Assessment Framework'),\n",
       " (0.0406346645234315, 'Science diplomacy and pandemics')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_tfidf(\"When COVID-19 will be ended?\", wiki_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.26854372399806203, 'COVID-19 pandemic')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_tfidf(\"coronavirus\", wiki_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The COVID-19 pandemic, also known as the coronavirus pandemic, is an ongoing pandemic of coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The disease was first identified in December 2019 in Wuhan, China. The outbreak was declared a Public Health Emergency of International Concern in January 2020, and a pandemic in March 2020. As of 17 October 2020, more than 39.5 million cases have been confirmed, with more than 1.1 million deaths attributed to COVID-19.\n",
      "\n",
      "Common symptoms include fever, cough, fatigue, breathing difficulties, and loss of smell. Complications may include pneumonia and acute respiratory distress syndrome. The incubation period is typically around five days but may range from one to 14 days. There are several vaccine candidates in development, although none have proven their safety and efficacy. There is no known specific antiviral medication, so primary treatment is currently symptomatic.\n",
      "Recommended preventive measures include hand washing, covering mouth or wearing face mask when sneezing or coughing, social distancing, disinfecting surfaces, ventilation and air-filtering, and monitoring and self-isolation if exposed or symptomatic. Travel restrictions, lockdowns, workplace hazard controls, and facility closures have been implemented. Many places have also worked to increase testing capacity and trace contacts of the infected. These have caused social and economic disruption, including the largest global recession since the Great Depression. Extreme poverty and global famines are affecting hundreds of millions, inflamed by supply shortages. Many events, the environment and education systems have also been affected. Misinformation about the virus has circulated globally. There have been many incidents of xenophobia and racism against Chinese people and against those perceived as being Chinese or as being from areas with high infection rates.\n"
     ]
    }
   ],
   "source": [
    "for doc in wiki_data:\n",
    "    if doc[\"title\"] == \"COVID-19 pandemic\":\n",
    "        print(doc[\"text\"])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
